{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ’” Heart Failure Prediction - Machine Learning Classifier\n",
    "Building with Python and Scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='indice'></a>\n",
    "### Indice\n",
    "[Problem Definition](#introduction)<br>\n",
    "[Step 0.0:  Setting the Engine Tools](#step0)<br>\n",
    "$\\;\\;\\;\\;\\;$[Step 0.1:  Importing Required Python Libraries](#step0.1)<br>\n",
    "$\\;\\;\\;\\;\\;$[Step 0.2:  Display Setting](#step0.2)<br>\n",
    "$\\;\\;\\;\\;\\;$[Step 0.3: Defining Functions and Dictionaries](#step0.3)<br>\n",
    "[Step 1.0: Data Extraction](#step1)<br>\n",
    "$\\;\\;\\;\\;\\;$[Step 1.1: Downloading the Data](#step1.1)<br>\n",
    "$\\;\\;\\;\\;\\;$[Step 1.2: Loading the DataFrame](#step1.2)<br>\n",
    "$\\;\\;\\;\\;\\;$[Step 1.3: DataFrame Display](#step1.3)<br>\n",
    "[Step 2.0: Data Preparation](#step2)<br>\n",
    "$\\;\\;\\;\\;\\;$[Step 2.1: DataFrame Description](#step2.1)<br>\n",
    "$\\;\\;\\;\\;\\;$[Step 2.2: Quantifying Cardinality](#step2.2)<br>\n",
    "$\\;\\;\\;\\;\\;$[Step 2.3: Removing Duplicates](#step2.3)<br>\n",
    "$\\;\\;\\;\\;\\;$[Step 2.4: Removing Irrelevant Data](#step2.4)<br>\n",
    "$\\;\\;\\;\\;\\;$[Step 2.5: Fixing Structural Errors](#step2.5)<br>\n",
    "$\\;\\;\\;\\;\\;$[Step 2.6: Detecting Outliers](#step2.6)<br>\n",
    "$\\;\\;\\;\\;\\;$[Step 2.7: Handling Missing Data](#step2.7)<br>\n",
    "$\\;\\;\\;\\;\\;$[Step 2.8: One Hot Encoding](#step2.8)<br>\n",
    "[Step 3.0: Data Exploration/Visualization](#step3)<br>\n",
    "$\\;\\;\\;\\;\\;$[Step 3.1: Descriptive Statistics](#step3.1)<br> \n",
    "$\\;\\;\\;\\;\\;$[Step 3.2: Feature Selection ](#step3.2)<br> \n",
    "$\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;$[Step 3.2.1: Univariate Selction](#step3.2.1)<br> \n",
    "$\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;$[Step 3.2.2: Feature Importance](#step3.2.2)<br> \n",
    "$\\;\\;\\;\\;\\;$[Step 3.3: Features Correlation](#step3.3)<br> \n",
    "[Step 4.0: Predictive Modeling](#step4)<br>\n",
    "[Step 5.0: Model Validation](#step5)<br>\n",
    "[Deployment of the Solution](#deployment)<br>\n",
    "[Acknowledgements](#acknowledgements)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='introduction'></a>\n",
    "## Problem Definition\n",
    "\n",
    "Due to unhealthy eating habits and the sedentary lifestyle of modern people, heart diseases (which refers any condition that affects the structure or function of the heart) is one of the major concerns of our society. In 2020, about 697,000 Americans died from heart diseases ([CDC heart disease](https://www.cdc.gov/heartdisease/about.htm)). That means one in every five deaths was due to heart disease. It's known that early detection of these diseases can cause a reduction in the mortality rate from them. Although, there are instruments and clinical tests that can predict heart disease. The high cost and difficultly of continually monitoring all of the sufferers of heart disease, calls for alternative solutions. In this work, we will use data analysis and machine learning tools to find the characteristics that lead a certain person to have or not to have heart disease. The data<a name=\"cite_ref-1\"></a>[<sup>[1]</sup>](#cite_note-1) consists of five databases already available independently ([UCI Machine Learning databases](https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/)). It contains 918 observations<a name=\"cite_ref-2\"></a>[<sup>[2]</sup>](#cite_note-2) with 12 features, including the predicted feature (see the table below). \n",
    "\n",
    "|Index| Feature |Description | Domain |\n",
    "|:-|  :-|:- | :- | \n",
    "|0|Age  |age of the patient    | years|\n",
    "|1|Sex       |sex of the patient|M: Male, F: Female|\n",
    "|2|ChestPainType|chest pain type| TA: Typical Angina, ATA: Atypical Angina, NAP: Non-Anginal Pain, ASY: Asymptomatic|\n",
    "|3|RestingBP|resting blood pressure (mm Hg)| --- |\n",
    "|4|Cholesterol|serum cholesterol (mm/dl)| --- |\n",
    "|5|FastingBS|fasting blood sugar| 1: if FastingBS > 120 mg/dl, 0: otherwise|\n",
    "|6|RestingECG|resting electrocardiogram results  |Normal: Normal, ST: having ST-T wave abnormality (T wave inversions and/or ST elevation or depression of > 0.05 mV), LVH: showing probable or definite left ventricular hypertrophy by Estes' criteria |\n",
    "|7|MaxHR|maximum heart rate achieved | Numeric value between 60 and 202\n",
    "|8|ExerciseAngina|exercise-induced angina |Y: Yes, N: No|\n",
    "|9|Oldpeak|ST depression induced by exercise relative to rest\t| Numeric value measured in depression|\n",
    "|10|ST_Slope|the slope of the peak exercise ST segment|Up: upsloping, Flat: flat, Down: downsloping |\n",
    "|11|HeartDisease|output class  | 1: heart disease, 0: normal|\n",
    "\n",
    "Note that the \"HeartDisease\" field refers to the presence of heart disease in the patient.  We will examine trends and correlations within available data by determining which features are essential concerning the presence of heart disease. Finally, we will compare different classification machine learning algorithms and find the efficient one for considering as a \"heart disease classifier\".\n",
    "\n",
    "\n",
    "<a name=\"cite_note-1\"></a>1. [](#cite_ref-1) fedesoriano. (September 2021). Heart Failure Prediction Dataset. Retrieved [August 7, 2022] from https://www.kaggle.com/fedesoriano/heart-failure-prediction.\n",
    "\n",
    "<a name=\"cite_note-2\"></a>2. [](#cite_ref-2) Cleveland: 303 observations, Hungarian: 294 observations, Switzerland: 123 observations, Long Beach VA: 200 observations and Stalog (Heart) Data Set: 270 observations<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[back to the top](#indice)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step0'></a>\n",
    "## Step 0.0:  Setting the Engine Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step0.1'></a>\n",
    "### Step 0.1: Importing Required Python Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas  as pd  # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import numpy   as np  # linear algebra\n",
    "import seaborn as sns # statistical data visualization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-22T02:15:39.838880Z",
     "iopub.status.busy": "2022-07-22T02:15:39.838536Z",
     "iopub.status.idle": "2022-07-22T02:15:39.842663Z",
     "shell.execute_reply": "2022-07-22T02:15:39.841767Z",
     "shell.execute_reply.started": "2022-07-22T02:15:39.838857Z"
    }
   },
   "outputs": [],
   "source": [
    "import os # OS routines\n",
    "import matplotlib.pyplot as plt # a MATLAB-like way of plotting (a state-based interface to matplotlib). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import opendatasets as od # collection of downloadable  datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions from Scikit-Learn Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-22T02:15:39.844752Z",
     "iopub.status.busy": "2022-07-22T02:15:39.843799Z",
     "iopub.status.idle": "2022-07-22T02:15:40.265142Z",
     "shell.execute_reply": "2022-07-22T02:15:40.263974Z",
     "shell.execute_reply.started": "2022-07-22T02:15:39.844710Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics         import accuracy_score\n",
    "from sklearn.linear_model    import LogisticRegression, SGDClassifier\n",
    "from sklearn.tree            import DecisionTreeClassifier\n",
    "from sklearn.ensemble        import GradientBoostingClassifier, RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.neighbors       import KNeighborsClassifier\n",
    "from sklearn.naive_bayes     import GaussianNB\n",
    "from sklearn.svm             import LinearSVC, SVC\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.preprocessing   import StandardScaler, LabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step0.2'></a>\n",
    "### Step 0.2: Display Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-22T02:15:40.268610Z",
     "iopub.status.busy": "2022-07-22T02:15:40.268088Z",
     "iopub.status.idle": "2022-07-22T02:15:40.274937Z",
     "shell.execute_reply": "2022-07-22T02:15:40.273638Z",
     "shell.execute_reply.started": "2022-07-22T02:15:40.268585Z"
    }
   },
   "outputs": [],
   "source": [
    "# Disabling Scientific Notation \n",
    "pd.options.display.float_format = '{:.2f}'.format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-22T02:15:40.268610Z",
     "iopub.status.busy": "2022-07-22T02:15:40.268088Z",
     "iopub.status.idle": "2022-07-22T02:15:40.274937Z",
     "shell.execute_reply": "2022-07-22T02:15:40.273638Z",
     "shell.execute_reply.started": "2022-07-22T02:15:40.268585Z"
    }
   },
   "outputs": [],
   "source": [
    "# Opening Figures on Screen\n",
    "%matplotlib inline  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-22T02:15:40.268610Z",
     "iopub.status.busy": "2022-07-22T02:15:40.268088Z",
     "iopub.status.idle": "2022-07-22T02:15:40.274937Z",
     "shell.execute_reply": "2022-07-22T02:15:40.273638Z",
     "shell.execute_reply.started": "2022-07-22T02:15:40.268585Z"
    }
   },
   "outputs": [],
   "source": [
    "# Setting Plots Parameters\n",
    "sns.set_style('whitegrid') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step0.3'></a>\n",
    "### Step 0.3: Defining Functions and Dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-22T02:15:44.579574Z",
     "iopub.status.busy": "2022-07-22T02:15:44.578369Z",
     "iopub.status.idle": "2022-07-22T02:15:44.587333Z",
     "shell.execute_reply": "2022-07-22T02:15:44.585833Z",
     "shell.execute_reply.started": "2022-07-22T02:15:44.579547Z"
    }
   },
   "outputs": [],
   "source": [
    "def detect_outliers(data): \n",
    "    '''Returns a list of the outiliers detcted (points that lie outside the range defined by the quartiles +/- 1.5 * IQR, where IQR is Interquartile Range):\n",
    "    \n",
    "    detect_outliers(data)\n",
    "    >>> outliers\n",
    "    '''\n",
    "    outliers = []\n",
    "    for col in nocategorical_features:\n",
    "        q1, q3= np.percentile(sorted(data[col]),[25,75])\n",
    "        iqr = q3 - q1\n",
    "       \n",
    "        lower_bound = q1 - (1.5 * iqr) \n",
    "        upper_bound = q3 + (1.5 * iqr)\n",
    "        \n",
    "        outliers.append(data[(data[col]<lower_bound ) | (data[col]> upper_bound)].index.values)\n",
    "   \n",
    "    outliers = tuple(outliers)\n",
    "    outliers = flatten(outliers)\n",
    "    return outliers      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detect_outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-22T02:15:44.579574Z",
     "iopub.status.busy": "2022-07-22T02:15:44.578369Z",
     "iopub.status.idle": "2022-07-22T02:15:44.587333Z",
     "shell.execute_reply": "2022-07-22T02:15:44.585833Z",
     "shell.execute_reply.started": "2022-07-22T02:15:44.579547Z"
    }
   },
   "outputs": [],
   "source": [
    "def drop_outliers(data, outliers): \n",
    "    '''Returns the data without the outliers rows:\n",
    "    \n",
    "    drop_outliers(data, outliers)\n",
    "    >>> data\n",
    "    '''\n",
    "    data = data.drop(df.index[outliers])\n",
    "    data.reset_index(drop=True, inplace=True)\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-22T02:15:44.579574Z",
     "iopub.status.busy": "2022-07-22T02:15:44.578369Z",
     "iopub.status.idle": "2022-07-22T02:15:44.587333Z",
     "shell.execute_reply": "2022-07-22T02:15:44.585833Z",
     "shell.execute_reply.started": "2022-07-22T02:15:44.579547Z"
    }
   },
   "outputs": [],
   "source": [
    "def flatten(xss):\n",
    "    '''Returns a single list from a composed list:\n",
    "    \n",
    "    flatten(xss)\n",
    "    >>> [x for xs in xss for x in xs]\n",
    "    '''  \n",
    "    return [x for xs in xss for x in xs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_statistics(data):\n",
    "    '''\n",
    "    Returns a background gradient of the statistics:\n",
    "    \n",
    "    gradient_statistics(data)\n",
    "    >>> data.describe().style.background_gradient(cmap='Reds')\n",
    "    '''\n",
    "    return data.describe().style.background_gradient(cmap='Reds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def boxenplot_generate(data):\n",
    "    '''\n",
    "    Returns a boxenplot:\n",
    "    \n",
    "    boxenplot_generate(data)\n",
    "    >>> sns.boxenplot(data=data)\n",
    "    >>> plt.figure(figsize=(12,6))\n",
    "    >>> plt.xlabel(\"No Categorical Features\") # add x label\n",
    "    >>> plt.ylabel(\"Entries\") # add y label\n",
    "    >>> plt.title(\"Shape of the Distribution\") # add a histogram title\n",
    "    >>> sns.boxenplot(data=data)\n",
    "    '''\n",
    "    plt.figure(figsize=(12,6))\n",
    "    plt.xlabel(\"No Categorical Features\") # add x label\n",
    "    plt.ylabel(\"Entries\") # add y label\n",
    "    plt.title(\"Shape of the Distribution\") # add a histogram title\n",
    "    sns.boxenplot(data=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotbar_generate(data):\n",
    "    '''\n",
    "    Returns a plotbar:\n",
    "    \n",
    "    plotbar_generate(data)\n",
    "    >>> data.nunique().plot.bar(figsize=(8,4), color=plt.cm.Paired(np.arange(len(data))))\n",
    "    >>> data.nunique().plot.bar(figsize=(8,4), color=plt.cm.Paired(np.arange(len(data)))) # change the figure size with the figsize argument\n",
    "    >>> plt.xlabel('Features') # add x label\n",
    "    >>> plt.ylabel('Number of Unique Categories') # add y label\n",
    "    >>> plt.title('Features Cardinality') # add a histogram title\n",
    "    '''\n",
    "    data.nunique().plot.bar(figsize=(8,4), color=plt.cm.Paired(np.arange(len(data)))) # change the figure size with the figsize argument\n",
    "    plt.xlabel('Features') # add x label\n",
    "    plt.ylabel('Number of Unique Categories') # add y label\n",
    "    plt.title('Features Cardinality') # add a histogram title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-22T02:15:41.452324Z",
     "iopub.status.busy": "2022-07-22T02:15:41.451755Z",
     "iopub.status.idle": "2022-07-22T02:15:41.460865Z",
     "shell.execute_reply": "2022-07-22T02:15:41.460224Z",
     "shell.execute_reply.started": "2022-07-22T02:15:41.452298Z"
    }
   },
   "outputs": [],
   "source": [
    "def cardinality_quantifying(data, num_cord=10):\n",
    "    '''\n",
    "    Returns the categorical columns with relatively low cardinality (< 10), \n",
    "    nocategorical columns, numerical column and object columns:\n",
    "    \n",
    "    cardinality_quantifying(data)\n",
    "    >>> categorical_features, nocategorical_features, numerical_features, object_features\n",
    "    '''\n",
    "    numerical_features = data.select_dtypes(\"number\").columns\n",
    "    object_features = data.select_dtypes(\"object\").columns\n",
    "    numerical_features= list(set(numerical_features))\n",
    "    object_features= list(set(object_features))\n",
    "   \n",
    "    categorical_features = []\n",
    "    nocategorical_features = []\n",
    "\n",
    "    for col in data.columns:\n",
    "        if data[col].nunique() < num_cord:\n",
    "            categorical_features.append(col)\n",
    "        else:\n",
    "            nocategorical_features.append(col)            \n",
    "    \n",
    "    return categorical_features, nocategorical_features, numerical_features, object_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Scaling(df_train, df_test):\n",
    "    '''\n",
    "    Returns a standard scaled tuple of DataFrame:\n",
    "    \n",
    "    Scaling(df_train, df_test)\n",
    "    >>> df_train, df_test\n",
    "    '''\n",
    "    scaler = StandardScaler()\n",
    "    nocategorical_features=cardinality_quantifying(df_train)[1]\n",
    "    df_train[nocategorical_features] = scaler.fit_transform(df_train[nocategorical_features])\n",
    "    df_test[nocategorical_features] = scaler.transform(df_test[nocategorical_features])\n",
    "\n",
    "    return df_train, df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MLC_report(data, dict_classifiers):\n",
    "    '''\n",
    "    Returns a report classification DataFrame:\n",
    "    \n",
    "    MLC_report(data, dict_classifiers)\n",
    "    >>> scores_data\n",
    "    '''\n",
    "    scores_dict = {'models': [],\n",
    "                   'train accuracy (%)':[],\n",
    "                  # 'test accuracy (%)':[],\n",
    "                   'pred accuracy (%)':[]\n",
    "                  }\n",
    "    models_index = list(scores_dict)[0]\n",
    "    train_index = list(scores_dict)[1]\n",
    "#     test_index = list(scores_dict)[2]\n",
    "    pred_index = list(scores_dict)[2]\n",
    "    \n",
    "    #nocategorical_features=cardinality_quantifying(data)[1]\n",
    "\n",
    "    for model, model_instantiation in dict_classifiers.items():\n",
    "        \n",
    "        X = data.drop(TARGET_col, axis=1)\n",
    "        y = data[TARGET_col]\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.2, random_state=42)\n",
    "        \n",
    "        X_train, X_test = Scaling(X_train, X_test)\n",
    "        \n",
    "\n",
    "        model_instantiation.fit(X_train,y_train)\n",
    "\n",
    "        score_train = model_instantiation.score(X_train,y_train)\n",
    "        # print(f\"accuracy train: {score_train * 100:.2f}%\")\n",
    "\n",
    "#         score_test = model_instantiation.score(X_test,y_test)\n",
    "#         #print(f\"accuracy (test): {score_test * 100:.2f}%\")\n",
    "        \n",
    "        y_pred = model_instantiation.predict(X_test)\n",
    "        score_pred = accuracy_score(y_test,y_pred)\n",
    "        #print(f\"accuracy (pred): {score_pred * 100:.2f}%\")\n",
    "        \n",
    "        scores_dict[train_index].append(round(score_train*100,2))\n",
    "#         scores_dict[test_index].append(round(score_test*100,2))\n",
    "        scores_dict[pred_index].append(round(score_pred*100,2))\n",
    "        scores_dict[models_index].append(model)\n",
    "        \n",
    "    scores_data = pd.DataFrame(scores_dict)\n",
    "    scores_data.sort_values(by=pred_index, ascending=False, inplace=True)\n",
    "    return scores_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def features_score1(data):\n",
    "    '''\n",
    "    Returns the data withiout outiliers rows:\n",
    "    \n",
    "    drop_outliers(data, outliers)\n",
    "    >>> data\n",
    "    '''\n",
    "#     X = data.iloc[:,0:13]  #independent columns\n",
    "#     y = data.iloc[:,-1]    #target column \n",
    "    X = data.drop(TARGET_col, axis=1)\n",
    "    y = data[TARGET_col]\n",
    "    #apply SelectKBest class to extract top best features\n",
    "    bestfeatures = SelectKBest(score_func=chi2, k=10)\n",
    "    fit = bestfeatures.fit(X,y)\n",
    "    dfscores = pd.DataFrame(fit.scores_)\n",
    "    dfcolumns = pd.DataFrame(X.columns)\n",
    "        \n",
    "    #concat two dataframes for better visualization \n",
    "    featureScores = pd.concat([dfcolumns,dfscores],axis=1)\n",
    "    featureScores.columns = ['Specs','Score']  #naming the dataframe columns\n",
    "    return featureScores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def features_score(data):\n",
    "    '''\n",
    "    Returns the data withiout outiliers rows:\n",
    "    \n",
    "    drop_outliers(data, outliers)\n",
    "    >>> data\n",
    "    '''\n",
    "\n",
    "    X = data.drop(TARGET_col, axis=1)\n",
    "    y = data[TARGET_col]\n",
    "   \n",
    "    #apply SelectKBest class to extract top best features\n",
    "    bestfeatures = SelectKBest(score_func=chi2, k=10)\n",
    "    fit = bestfeatures.fit(X,y)\n",
    "    dfscores = pd.DataFrame(fit.scores_)\n",
    "    dfcolumns = pd.DataFrame(X.columns)\n",
    "    \n",
    "    print(fit.scores_)\n",
    "    \n",
    "    #concat two dataframes for better visualization \n",
    "    featureScores = pd.concat([dfcolumns,dfscores],axis=1)\n",
    "    featureScores.columns = ['Specs','Score']  #naming the dataframe columns\n",
    "    print(featureScores.nlargest(12,'Score'))  #print best features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def features_importance(data):\n",
    "    '''\n",
    "    Returns the data withiout outiliers rows:\n",
    "    \n",
    "    drop_outliers(data, outliers)\n",
    "    >>> data\n",
    "    '''\n",
    "    model = ExtraTreesClassifier()\n",
    "    # X = data.iloc[:,0:13]  #independent columns\n",
    "    # y = data.iloc[:,-1]    #target column \n",
    "    X = df.drop(TARGET_col, axis=1)\n",
    "    y = df[TARGET_col]\n",
    "    model.fit(X,y)\n",
    "    print(model.feature_importances_) #use inbuilt class feature_importances of tree based classifiers\n",
    "    #plot graph of feature importances for better visualization\n",
    "    feat_importances = pd.Series(model.feature_importances_, index=X.columns)\n",
    "    feat_importances.nlargest(13).plot(kind='barh')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distribution_feature(data):\n",
    "    '''\n",
    "    Returns the data withiout outiliers rows:\n",
    "    \n",
    "    drop_outliers(data, outliers)\n",
    "    >>> data\n",
    "    '''\n",
    "    for col in data.columns:\n",
    "        if col != TARGET_col:\n",
    "            plt.figure(figsize=(8,4))\n",
    "            plt.title(f\"Distribution of Heart Diseases by {col}\", fontsize=16)\n",
    "            if col in categorical_features:\n",
    "                sns.countplot(x=data[col], hue=data[TARGET_col])\n",
    "            else:\n",
    "                sns.histplot(data = data, x= data[col], hue=TARGET_col, kde=True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots(nrows = 2,ncols = 2,figsize = (10,9.75))\n",
    "# for i in range(len(numerical_features) - 1):\n",
    "#     plt.subplot(2,2,i+1)\n",
    "#     sns.distplot(data[numerical_features[i]])\n",
    "#     title = 'Distribution : ' + numerical_features[i]\n",
    "#     plt.title(title)\n",
    "# plt.show()\n",
    "\n",
    "# plt.figure(figsize = (4.75,4.55))\n",
    "# sns.distplot(df1[numerical_features[len(numerical_features) - 1]],kde_kws = {'bw' : 1})\n",
    "# title = 'Distribution : ' + numerical_features[len(numerical_features) - 1]\n",
    "# plt.title(title);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_classifiers = {\n",
    "    \"Gradient Boosting Classifier\": GradientBoostingClassifier(),\n",
    "    \"Decision Tree Classifier\": DecisionTreeClassifier(),\n",
    "    \"Random Forest Classifier\": RandomForestClassifier(n_estimators=100, n_jobs=-1, random_state=0),\n",
    "    \"K Neighbors Classifier\": KNeighborsClassifier(),\n",
    "    \"SGD Classifier\": SGDClassifier(),\n",
    "    \"Gaussian NB\": GaussianNB(),\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000,solver='lbfgs'),\n",
    "    \"Linear SVM\": SVC(probability=True, kernel='linear'),\n",
    "    \"Linear SVC\": LinearSVC(dual=False),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[back to the top](#indice)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step1'></a>\n",
    "## Step 1.0:  Data Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step1.1'></a>\n",
    "### Step 1.1: Downloading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "od.download(\"https://www.kaggle.com/datasets/fedesoriano/heart-failure-prediction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step1.2'></a>\n",
    "### Step 1.2: Loading the DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-20T05:43:41.794119Z",
     "iopub.status.busy": "2022-07-20T05:43:41.793657Z",
     "iopub.status.idle": "2022-07-20T05:43:41.802447Z",
     "shell.execute_reply": "2022-07-20T05:43:41.800983Z",
     "shell.execute_reply.started": "2022-07-20T05:43:41.794081Z"
    }
   },
   "source": [
    "The CSV files can be loaded into a DataFrame as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-22T02:15:40.298578Z",
     "iopub.status.busy": "2022-07-22T02:15:40.297430Z",
     "iopub.status.idle": "2022-07-22T02:15:40.318416Z",
     "shell.execute_reply": "2022-07-22T02:15:40.317529Z",
     "shell.execute_reply.started": "2022-07-22T02:15:40.298537Z"
    }
   },
   "outputs": [],
   "source": [
    "# Path of the file to read\n",
    "file_path = \"~/PythonProgramming/JupyterProjects/Kaggle/kaggleDataSets/HeartDiseaseDataset/heart-failure-prediction/heart.csv\"\n",
    "data = pd.read_csv(file_path) # reading csv file \n",
    "print(f'DataFrame successfully loaded!\\npath: \"{file_path}\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step1.3'></a>\n",
    "### Step 1.3: DataFrame Display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After loading the data, print a sample and some aditional information to see what we're working with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The are a total of 918 entries and 12 features. Note that the data frame is correctly indexed, that is, without an index feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>TIP</b><br>Below are some helpful methods:\n",
    "```python\n",
    "data.head() # display the first five rows\n",
    "data.tail() # display the last five rows\n",
    "data.shape  # check out the dimension of the dataset\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[back to the top](#indice)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step2'></a>\n",
    "## Step 2.0: Data Preparation\n",
    "\n",
    "In this section, we will focus on the preparation of the data: cleaning, normalizing, and transforming\n",
    "data into an optimized data set, that is, in a prepared format, normally tabular, suitable for the methods of\n",
    "analysis that has been scheduled during the design phase. For this dataset, as the number of features are less, we can manually check the dataset as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before starting the next step, let's make a copy to keep safe the original data and save the target name for ease of use within some functions posteriorly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data.copy(deep=True) # making a deep copy of DataFrame and save it to df\n",
    "TARGET_col = 'HeartDisease' # saving the target name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step2.1'></a>\n",
    "### Step 2.1: Data Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Printing general informations, including the index dtype and columns, non-null values and memory usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-22T02:15:40.346463Z",
     "iopub.status.busy": "2022-07-22T02:15:40.345644Z",
     "iopub.status.idle": "2022-07-22T02:15:40.372182Z",
     "shell.execute_reply": "2022-07-22T02:15:40.370413Z",
     "shell.execute_reply.started": "2022-07-22T02:15:40.346423Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-20T05:26:18.413275Z",
     "iopub.status.busy": "2022-07-20T05:26:18.412843Z",
     "iopub.status.idle": "2022-07-20T05:26:18.421529Z",
     "shell.execute_reply": "2022-07-20T05:26:18.420119Z",
     "shell.execute_reply.started": "2022-07-20T05:26:18.413243Z"
    }
   },
   "source": [
    "The are a total of 86.2 KB of memory usage. No columns have missing values (Non-Null Count = 918 entries). Note that there are five object-like features. It will be necessary to address these features later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-20T00:00:49.857656Z",
     "iopub.status.busy": "2022-07-20T00:00:49.857218Z",
     "iopub.status.idle": "2022-07-20T00:00:49.864865Z",
     "shell.execute_reply": "2022-07-20T00:00:49.863346Z",
     "shell.execute_reply.started": "2022-07-20T00:00:49.857624Z"
    }
   },
   "source": [
    "<b>TIP</b><br>Below are some helpful methods:\n",
    "```python\n",
    "data.dtypes                  # look at the data types for each column  \n",
    "data.columns.values          # return an array of column names\n",
    "data.columns.values.tolist() # return a list of column names\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step2.2'></a>\n",
    "### Step 2.2: Quantifying Cardinality "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before starting the preparation of the data itself, let's classify the features as categorical or non-categorical and numeric or non-numeric. Remembering that the cardinality of a feature is given by its number of unique categories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotbar_generate(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will define as a categorical variable those with a cardinality lower than 10. All others will be considered non-categorical. Also, we will select numeric and non-numeric features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-22T02:15:41.430320Z",
     "iopub.status.busy": "2022-07-22T02:15:41.429749Z",
     "iopub.status.idle": "2022-07-22T02:15:41.438178Z",
     "shell.execute_reply": "2022-07-22T02:15:41.436756Z",
     "shell.execute_reply.started": "2022-07-22T02:15:41.430289Z"
    }
   },
   "outputs": [],
   "source": [
    "categorical_features, nocategorical_features, numerical_features, object_features = cardinality_quantifying(data, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the classification result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"categorical:\\n\", categorical_features)\n",
    "print(\"no categorical:\\n\", nocategorical_features)\n",
    "print(\"numerical:\\n\", numerical_features)\n",
    "print(\"object:\\n\", object_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following we''ll work with some of the central machine learning data cleaning steps (see [What Is Data Cleaning: A Practical Guide](https://deepchecks.com/what-is-data-cleaning/))\n",
    ". We will use tools and automation to reduce the unnecessary overhead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step2.3'></a>\n",
    "### Step 2.3: Removing Duplicates\n",
    "\n",
    "Duplicate entries are always problematic in data analysis. If an entry that appears more than once and is given disproportionate weight during training can result in great results for the training data and serious discrepancy in test time. Let's start by checking if there are duplicate entries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-22T02:15:40.421483Z",
     "iopub.status.busy": "2022-07-22T02:15:40.420959Z",
     "iopub.status.idle": "2022-07-22T02:15:40.431664Z",
     "shell.execute_reply": "2022-07-22T02:15:40.430322Z",
     "shell.execute_reply.started": "2022-07-22T02:15:40.421449Z"
    }
   },
   "outputs": [],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no duplicate entries. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>TIP</b><br>If there are duplicate entries it is necessary to remove them: \n",
    "```python\n",
    "data.drop_duplicates(inplace=True)  # remove duplicates, if any.\n",
    "data.reset_index(drop=True, inplace=True) # reset the DataFrame index\n",
    "```\n",
    "It's necessary to set inplace as True so that the changes are from the same DataFrame and drop as True to drop the old index kept by the reset_index() function.\n",
    "\n",
    "Below are some useful methods for droping rows and columns:\n",
    "```python\n",
    "subset_1 = data.drop(data.index[[1,7,9]])  # drop the 2nd, 8th, and 10th rows\n",
    "subset_2 = data.drop(data.index[range(1,11)])  # drop all rows between 2nd to 10th rows\n",
    "subset_3 = data.drop([\"col1\", \"col2\"], axis=1)  # drop col1 and col2\n",
    "subset_4 = data.iloc[:100, :]  # a subset of the first 100 rows of the original data\n",
    "subset_5 = data.iloc[:, :3]  # a subset of the first 3 columns of the original data\n",
    "subset_6 = data.iloc[:100, :15]  # a subset of the first 100 rows and the first 15 columns\n",
    "subset_7 = data[[\"col1\", \"col2\", \"col3\"]]  # a subset contains features col1, col2, and col3\n",
    "subset_8 = data.sample(n=1000) # a random sample of size 1000 without replacement (replace = False (Default))\n",
    "subset_9 = data.sample(frac=0.1, replace=True) # a random sample of 10% of the original data with replacement\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step2.4'></a>\n",
    "### Step 2.4: Removing Irrelevant Data \n",
    "It is not uncommon for the data to come from diverse sources and consequently there is a probability of unnecessary entries or even entries that don't belong. At the first analysis, we don't see in the DataFrame any irrelevant data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step2.5'></a>\n",
    "### Step 2.5: Fixing Structural Errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is not uncommon to see data with similar names  (for example, an underscore or a capital letter) or incorrectly filled (for example, a categorical value outside of the predefined range). Let's see if the unique values of the categorical features have any \"strange\" entry:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Sex:\", df['Sex'].unique())\n",
    "print(\"ChestPainType:\", df['ChestPainType'].unique())\n",
    "print(\"FastingBS:\", df['FastingBS'].unique())\n",
    "print(\"RestingECG:\", df['RestingECG'].unique())\n",
    "print(\"ExerciseAngina:\", df['ExerciseAngina'].unique())\n",
    "print(\"ST_Slope:\", df['ST_Slope'].unique())\n",
    "print(\"HeartDisease:\", df['HeartDisease'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that there are no structural errors. For all features, the entries match the expected range (see table in the section [Problem Definition](#introduction)). Now let's see the range of the values of the no categorical features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(f\"Range of the values no categorical features\\n\")\n",
    "for col in df[nocategorical_features].columns:\n",
    "    print(f\"{col}: {df[col].min()} - {df[col].max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that there are values negative for the  \"Oldpeak\" feature that are incorrect entries, domain: 0-6.2 (see [A Hybrid Classification System for Heart Disease Diagnosis Based on the RFRS Method](https://www.hindawi.com/journals/cmmm/2017/8272091/))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Total number of \"Oldpeak\" negative entries:', df.loc[(df[\"Oldpeak\"]<0)].shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will treat these negative entries later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step2.6'></a>\n",
    "### Step 2.6: Detecting Outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outlier detection requires a deeper understanding of what the data should look like, and when entries should be ignored because they are inaccurate. To detect unwanted outliers we need to explore the ranges and possibilities for numerical and categorical data entries. In the case of the categorical data, we saw there are no outliers. Let's generate the background color of the descriptive statistics according to the each feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient_statistics(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note evidence of the existence of some outliers. The last quartiles of the \"RestingBP\", \"Cholesterol\" and \"MaxHR\" features seem to break out of their respective growth trends. The first quartiles of the \"RestingBP\" and \"Cholesterol\" features seem to breack out of their respective decrease trends:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Total number of \"RestingBP\" null entries:', df.loc[(df[\"RestingBP\"]==0)].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Total number of \"Cholesterol\" null entries:', df.loc[(df[\"Cholesterol\"]==0)].shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see in this first analysis that there are at least 173 entries outside the expected range of the features \"RestingBP\" and \"Cholesterol\". Drawing an enhanced box plot for the no categorical features provides more information about the shape of the distribution, particularly in the tails:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-22T02:15:44.215372Z",
     "iopub.status.busy": "2022-07-22T02:15:44.214485Z",
     "iopub.status.idle": "2022-07-22T02:15:44.409178Z",
     "shell.execute_reply": "2022-07-22T02:15:44.408511Z",
     "shell.execute_reply.started": "2022-07-22T02:15:44.215344Z"
    }
   },
   "outputs": [],
   "source": [
    "boxenplot_generate(df[nocategorical_features])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how clear the existence of outliers is. Mainly, null values for the \"Cholesterol\" feature. Now we will use InterQuartile Range (IQR) to see when a value is too far from the  middle data values. Will be considered an outilier a point which falls more than 1.5 times the interquartile range above the third quartile or below the first quartile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-22T02:15:44.665356Z",
     "iopub.status.busy": "2022-07-22T02:15:44.665071Z",
     "iopub.status.idle": "2022-07-22T02:15:44.678012Z",
     "shell.execute_reply": "2022-07-22T02:15:44.676168Z",
     "shell.execute_reply.started": "2022-07-22T02:15:44.665333Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "outliers=detect_outliers(df)\n",
    "print(\"There are\", len(outliers), \"outiliers.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's remove the outliers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-22T02:15:44.665356Z",
     "iopub.status.busy": "2022-07-22T02:15:44.665071Z",
     "iopub.status.idle": "2022-07-22T02:15:44.678012Z",
     "shell.execute_reply": "2022-07-22T02:15:44.676168Z",
     "shell.execute_reply.started": "2022-07-22T02:15:44.665333Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df=drop_outliers(df,outliers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again generating the background color of the descriptive statistics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe().style.background_gradient(cmap='Reds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that there still seems to be scattered data. We will keep the data as it is since we have already removed the outliers once. A further study could examine the consequences of performing yet another outliers remotion. There are some negative values for the \"Oldpeak\" feature remain. Let's remove them manually:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(df[df[\"Oldpeak\"] < 0].index)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "print('Total number of \"Oldpeak\" negative entries:', df.loc[(df[\"Oldpeak\"]<0)].shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>TIP</b><br>Below is an useful method for droping with multiples condictions: \n",
    "```python\n",
    "data = data.drop(data[(df[\"col1\"] < 0) & (df[\"col2\"] == \"M\")].index) # removes rows in the \"col1\" feature where there are negative values and in the \"col2\" feature where the values are M.\n",
    "data.reset_index(drop=True, inplace=True) # resets the DataFrame index\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Drawing an enhanced box plot for the no categorical features to see the final results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boxenplot_generate(df[nocategorical_features])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step2.7'></a>\n",
    "### Step 2.7: Handling Missing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Handling missing data  in  machine learning data cleaning steps is very important. To recheck if there are any missing values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-22T02:15:40.459713Z",
     "iopub.status.busy": "2022-07-22T02:15:40.459058Z",
     "iopub.status.idle": "2022-07-22T02:15:40.469556Z",
     "shell.execute_reply": "2022-07-22T02:15:40.468664Z",
     "shell.execute_reply.started": "2022-07-22T02:15:40.459673Z"
    }
   },
   "outputs": [],
   "source": [
    "data.isnull().values.any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we expect there are no missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-20T01:31:40.871295Z",
     "iopub.status.busy": "2022-07-20T01:31:40.870526Z",
     "iopub.status.idle": "2022-07-20T01:31:40.878688Z",
     "shell.execute_reply": "2022-07-20T01:31:40.876307Z",
     "shell.execute_reply.started": "2022-07-20T01:31:40.871253Z"
    }
   },
   "source": [
    "<b> TIP</b><br>Below are examples of how to use some useful methods to deal with Missing Values. \n",
    "\n",
    "Check information of the missing values:\n",
    "```python\n",
    "data.isnull()  # checking missing values\n",
    "data[\"col1\"].isnull().sum()  # return the number of missing values in col1\n",
    "data.notnull()  # checking non-missing values\n",
    "data.isnull().values.any()  # only want to know if there are any missing values\n",
    "data[\"col1\"].isnull().values.any()  # only want to know if there are any missing values in col1\n",
    "data.notnull().sum()  # knowling number of non-missing values for each variable\n",
    "data.isnull().sum().sum()  # knowing how many missing values in the data\n",
    "```\n",
    "Get information without missing values:\n",
    "\n",
    "```python\n",
    "data[data[\"col1\"].notnull()]  # the data contain rows that no missing values in col1\n",
    "data[data[\"col1\"].notnull() & data[\"col2\"].notnull()] # the data contain rows that no missing values in col1 and col2\n",
    "no_missing = data.dropna()  # drop missing values and assign the data to no_missing\n",
    "clean_missing_rows = data.dropna(how=\"all\")  # drop rows where all cells in the row in NA and assign the data to clean_missing_rows\n",
    "data.dropna(axis=1, how=\"all\")  # drop columns if they only contain missing values\n",
    "data.dropna(thresh=25)  # drop rows that contain less than 25 non-missing values\n",
    "```\n",
    "Fill in missing values:\n",
    "\n",
    "```python\n",
    "Fill_no = data.fillna(1000000)  # fill in missing with 1000000 and save the data to Fill_no\n",
    "Fill_str = data[\"col1\"].fillna(\"missing\")  # fill in missing with a string \"missing\" and save the data to Fill_str\n",
    "bikedata[\"col1\"].fillna(data[\"col1\"].mean(), inplace=True)  # fill missing values with the sample mean and save the changes to the original data\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step2.8'></a>\n",
    "### Step 2.8: One Hot Encoding\n",
    "\n",
    "Knowing that categorical (non-numeric) features are not handled by machine learning algorithms, we need to convert them to numeric features. This process is called  One Hot Encoding (OHE). To facilitate the analysis in the visualization step, we will save the data frame without the OHE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['Sex'] = LabelEncoder().fit_transform(df['Sex'])\n",
    "# df['ChestPainType'] = LabelEncoder().fit_transform(df['ChestPainType'])\n",
    "# df['RestingECG'] = LabelEncoder().fit_transform(df['RestingECG'])\n",
    "# df['ExerciseAngina'] = LabelEncoder().fit_transform(df['ExerciseAngina'])\n",
    "# df['ST_Slope'] = LabelEncoder().fit_transform(df['ST_Slope'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_noOHE = df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It was seen in the [step2.2](#step2.2) that the 2 features â€“ \"Sex\" and \"ExerciseAngina\" among the 5 total categorical features are binary i.e. they only take two values. It's possible, therefore, manually encode these using 0 and 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Sex'] = np.where(df['Sex'] == \"M\", 0, 1)\n",
    "df['ExerciseAngina'] = np.where(df['ExerciseAngina'] == \"N\", 0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this point we have: 0 to \"M\" (male) and 1 to \"F\" (female) for the feature \"Sex\" and 0 to \"N\" (no) and 1 to \"Y\" (yes) for the feature \"ExerciseAngina\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For resources with 3 or more, we will use the pandas get_dummies function. This function creates a new feature per label. For example, ChestPainType has 4 labels, therefore, 4 new features are created with values 0 or 1. As one of these features is redundant, the first attribute is deleted. To facilitate the analysis in the next ste we will save a copy of the complete data frame, that is, we will keep the first feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_OHE = pd.get_dummies(df, columns=['ChestPainType', 'RestingECG', 'ST_Slope'], drop_first=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.get_dummies(df, columns=['ChestPainType', 'RestingECG', 'ST_Slope'], drop_first=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a final result of the preparation step we have a DataFrame with a total of 701 entries and 16 features, with 39.8 KB of memory usage (to compare see [step2.1](#step2.1) ). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[back to the top](#indice)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yes = df[df['HeartDisease'] == 1].describe().T\n",
    "no = df[df['HeartDisease'] == 0].describe().T\n",
    "\n",
    "fig,ax = plt.subplots(nrows = 1,ncols = 2,figsize = (5,5))\n",
    "plt.subplot(1,2,1)\n",
    "sns.heatmap(yes[['mean']],annot = True,cmap = 'plasma',linewidths = 0.4,linecolor = 'black',cbar = False,fmt = '.2f')\n",
    "plt.title('Heart Disease');\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "sns.heatmap(no[['mean']],annot = True,cmap = 'plasma',linewidths = 0.4,linecolor = 'black',cbar = False,fmt = '.2f')\n",
    "plt.title('No Heart Disease');\n",
    "\n",
    "fig.tight_layout(pad = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step3'></a>\n",
    "## Step 3.0: Data Exploration/Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will search for data in a graphical or statistical presentation according to find patterns, connections, and relationships in the data. Data visualization is the best tool to highlight possible patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step3.1'></a>\n",
    "### Step 3.1: Descriptive Statistics \n",
    "Let's generate some descriptive statistics, excluding NaN values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-22T02:15:40.373608Z",
     "iopub.status.busy": "2022-07-22T02:15:40.373196Z",
     "iopub.status.idle": "2022-07-22T02:15:40.416842Z",
     "shell.execute_reply": "2022-07-22T02:15:40.415968Z",
     "shell.execute_reply.started": "2022-07-22T02:15:40.373584Z"
    }
   },
   "outputs": [],
   "source": [
    "df.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Except for the Cholesterol feature, which still seems to have scattered data about the center all, which can be considered in a further study, all the features are well distributed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step3.2'></a>\n",
    "### Step 3.2: Feature Selection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step3.2'></a>\n",
    "### Step 3.2: Feature Selection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step3.2'></a>\n",
    "### Step 3.2: Feature Selection "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step3.2.1'></a>\n",
    "#### Step 3.2.1: Univariate Selction\n",
    "\n",
    "The scikit-learn library provides the SelectKBest class that can be used to select the best features from the DataFrame. SelectKBest class selects a specific number of features in a suite of different statistical tests.\n",
    "In the following we use the chi-squared (chi2) statistical test for non-negative features to select 13 of the best features from the Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "features_score(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_score(df_OHE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step3.2.2'></a>\n",
    "#### Step 3.2.2: Feature Importance\n",
    "\n",
    "The significance of each feature of the dataset can be obtained by using the Model Characteristics property.\n",
    "Feature value gives a score for every function of the results, the higher the score the more significant or appropriate the performance variable is. Feature importance is the built-in class that comes with Tree-Based Classifiers. The Extra Tree Classifier will be used to extract the top features for the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "features_importance(df_OHE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step3.3'></a>\n",
    "### Step 3.3: Features Correlation\n",
    "\n",
    "Correlation shows whether the characteristics are related to each other or to the target variable. Correlation can be positive (increase in one value, the value of the objective variable increases) or negative (increase in one value, the value of the target variable decreased). From this heatmap we can observe that the â€˜cpâ€™ chest pain is highly related to the target variable. Compared to relation between other two variables we can say that chest pain contributes the most in prediction of presences of a heart disease. Medical emergency is a heart attack. A cardiac occurs usually when blood clot blocks blood flow to the cardiac. Tissue loses oxygen without blood and dies causing chest pain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A correlation could be positive, meaning both features move in the same direction, or negative, meaning that when one variableâ€™s value increases, the other featuresâ€™ values decrease. Correlation can also be neutral or zero, meaning that the features are unrelated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-22T02:15:40.516520Z",
     "iopub.status.busy": "2022-07-22T02:15:40.515965Z",
     "iopub.status.idle": "2022-07-22T02:15:40.526702Z",
     "shell.execute_reply": "2022-07-22T02:15:40.525733Z",
     "shell.execute_reply.started": "2022-07-22T02:15:40.516486Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,7))\n",
    "sns.heatmap(df.corr(), annot=True, cmap = 'Blues') # plot rectangular data as a color-encoded matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-22T02:15:40.529189Z",
     "iopub.status.busy": "2022-07-22T02:15:40.528802Z",
     "iopub.status.idle": "2022-07-22T02:15:41.428347Z",
     "shell.execute_reply": "2022-07-22T02:15:41.426953Z",
     "shell.execute_reply.started": "2022-07-22T02:15:40.529157Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,7))\n",
    "sns.heatmap(df_OHE.corr(), annot=True, cmap = 'Blues') # plot rectangular data as a color-encoded matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that none of the features were found to be strongly correlated with the target. There are four features (â€œcpâ€, â€œrestecgâ€, â€œthalachâ€, â€œslopeâ€) positively correlated with the target and nine negatively correlated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "distribution_feature(df_noOHE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Population distribution of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,4))\n",
    "plt.title(f\"Distribution\", fontsize=16)\n",
    "sns.histplot(data = df, x= df['Age'], hue=None, stat='density', kde=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # First create some toy data:\n",
    "# x = np.linspace(0, 2*np.pi, 400)\n",
    "# y = np.sin(x**2)\n",
    "\n",
    "# # Create just a figure and only one subplot\n",
    "# fig, ax = plt.subplots()\n",
    "# ax.plot(x, y)\n",
    "# ax.set_title('Simple plot')\n",
    "\n",
    "# # Create two subplots and unpack the output array immediately\n",
    "# f, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\n",
    "# ax1.plot(x, y)\n",
    "# ax1.set_title('Sharing Y axis')\n",
    "# ax2.scatter(x, y)\n",
    "\n",
    "# # Create four polar axes and access them through the returned array\n",
    "# fig, axs = plt.subplots(2, 2, subplot_kw=dict(projection=\"polar\"))\n",
    "# axs[0, 0].plot(x, y)\n",
    "# axs[1, 1].scatter(x, y)\n",
    "\n",
    "# # Share a X axis with each column of subplots\n",
    "# plt.subplots(2, 2, sharex='col')\n",
    "\n",
    "# # Share a Y axis with each row of subplots\n",
    "# plt.subplots(2, 2, sharey='row')\n",
    "\n",
    "# # Share both X and Y axes with all subplots\n",
    "# plt.subplots(2, 2, sharex='all', sharey='all')\n",
    "\n",
    "# # Note that this is the same as\n",
    "# plt.subplots(2, 2, sharex=True, sharey=True)\n",
    "\n",
    "# # Create figure number 10 with a single subplot\n",
    "# # and clears it if it already exists.\n",
    "# fig, ax = plt.subplots(num=10, clear=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f, axes = plt.subplots(2, 2, figsize=(7,7), sharex=True)\n",
    "\n",
    "# sns.distplot(df[df['Sex'] == 1]['Age'], axlabel=\"Homens Total\", ax=axes[0,0]) # Homens\n",
    "# sns.distplot(df[df['Sex'] == 0]['Age'], axlabel=\"Mulheres Total\", ax=axes[0,1]) # Mulheres\n",
    "\n",
    "# #Homens e mulheres que possuem DoenÃ§a CardÃ­aca\n",
    "# #df[df['sex'] == 1] <- Isso pode ser lido como: \"dados onde dados na coluna 'sex' seja igual a 1.\"\n",
    "# #Adicionalmente, onde estÃ¡ essa condiÃ§Ã£o, podem existir outras condiÃ§Ãµes. \n",
    "\n",
    "# homens_com_doenca_cardiaca = df[(df['Sex'] == 1) & (df['HeartDisease'] == 1)]['Age']\n",
    "# mulheres_com_doenca_cardiaca = df[(df['Sex'] == 0) & (df['HeartDisease'] == 1)]['Age']\n",
    "\n",
    "# sns.distplot(homens_com_doenca_cardiaca, axlabel=\"Homens com DoenÃ§a\", ax=axes[1,0]) \n",
    "# sns.distplot(mulheres_com_doenca_cardiaca, axlabel=\"Mulheres com DoenÃ§a\", ax=axes[1,1]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization of the relationship between Age, cholesterol and the target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.relplot(x='chol',y='age',hue='target', data=df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[back to the top](#indice)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step4'></a>\n",
    "## Step 4.0: Predictive Modeling\n",
    "\n",
    "Predictive modeling is a process used in data analysis to create or choose a suitable statistical model to\n",
    "predict the probability of a result.\n",
    "After exploring data you have all the information needed to develop the mathematical model that\n",
    "encodes the relationship between the data. These models are useful for understanding the system under\n",
    "study, and in a specific way they are used for two main purposes. The first is to make predictions about the\n",
    "data values produced by the system; in this case, you will be dealing with regression models. The second is\n",
    "to classify new data products, and in this case, you will be using classification models or clustering models.\n",
    "In fact, it is possible to divide the models according to the type of result that they produce:\n",
    "â€¢ Classification models: If the result obtained by the model type is categorical.\n",
    "â€¢ Regression models: If the result obtained by the model type is numeric.\n",
    "â€¢ Clustering models: If the result obtained by the model type is descriptive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(TARGET_col, axis=1)\n",
    "y = df[TARGET_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_classifiers[\"Logistic Regression\"].fit(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = dict_classifiers[\"Logistic Regression\"].predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list1 = [1, 2, 3, 4, 5]\n",
    "list2 = [123, 234, 456]\n",
    "d = {'a': [], 'b': []}\n",
    "d['a'].append(list1)\n",
    "d['a'].append(list2)\n",
    "print (d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[back to the top](#indice)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step5'></a>\n",
    "## Step 5.0: Model Validation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "REVERRRRR\n",
    "\n",
    "A maximum classification accuracy of 92.59% was achieved according to a jackknife cross-validation scheme. The results demonstrate that the performance of the proposed system is superior to the performances of previously reported classification techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scores_data=MLC_report(df, dict_classifiers)\n",
    "scores_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_data['models'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[back to the top](#indice)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='deployment'></a>\n",
    "## Deployment of the Solution \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "REVERRRRR\n",
    "\n",
    "A maximum classification accuracy of 92.59% was achieved according to a jackknife cross-validation scheme. The results demonstrate that the performance of the proposed system is superior to the performances of previously reported classification techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='acknowledgements'></a>\n",
    "## Acknowledgements\n",
    "Creators:\n",
    "1. Hungarian Institute of Cardiology. Budapest: Andras Janosi, M.D.\n",
    "2. University Hospital, Zurich, Switzerland: William Steinbrunn, M.D.\n",
    "3. University Hospital, Basel, Switzerland: Matthias Pfisterer, M.D.\n",
    "4. V.A. Medical Center, Long Beach and Cleveland Clinic Foundation: Robert Detrano, M.D., Ph.D.\n",
    "\n",
    "Donor: David W. Aha (aha '@' ics.uci.edu) (714) 856-8779"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[back to the top](#indice)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df1=features_score1(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "products_list = df1.nlargest(12,'Score').values.tolist()\n",
    "print(products_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = range(6)\n",
    "for n in x:\n",
    "    print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_features =[]\n",
    "for prod in range(10):\n",
    "    best_features.append(products_list[prod][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "products_list[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_features.append(TARGET_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df[best_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scores_data = MLC_report(df[best_features], dict_classifiers)\n",
    "scores_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sex = data[data['HeartDisease'] == 1]['Sex'].value_counts()\n",
    "sex = [sex[0] / sum(sex) * 100, sex[1] / sum(sex) * 100]\n",
    "\n",
    "cp = data[data['HeartDisease'] == 1]['ChestPainType'].value_counts()\n",
    "cp = [cp[0] / sum(cp) * 100,cp[1] / sum(cp) * 100,cp[2] / sum(cp) * 100,cp[3] / sum(cp) * 100]\n",
    "\n",
    "fbs = data[data['HeartDisease'] == 1]['FastingBS'].value_counts()\n",
    "fbs = [fbs[0] / sum(fbs) * 100,fbs[1] / sum(fbs) * 100]\n",
    "\n",
    "restecg = data[data['HeartDisease'] == 1]['RestingECG'].value_counts()\n",
    "restecg = [restecg[0] / sum(restecg) * 100,restecg[1] / sum(restecg) * 100,restecg[2] / sum(restecg) * 100]\n",
    "\n",
    "exang = data[data['HeartDisease'] == 1]['ExerciseAngina'].value_counts()\n",
    "exang = [exang[0] / sum(exang) * 100,exang[1] / sum(exang) * 100]\n",
    "\n",
    "slope = data[data['HeartDisease'] == 1]['ST_Slope'].value_counts()\n",
    "slope = [slope[0] / sum(slope) * 100,slope[1] / sum(slope) * 100,slope[2] / sum(slope) * 100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax,fig = plt.subplots(nrows = 4,ncols = 2,figsize = (15,15))\n",
    "colors = ['#F3ED13','#451FA4']\n",
    "\n",
    "plt.subplot(3,2,1)\n",
    "plt.pie(sex,labels = ['Male','Female'],autopct='%1.1f%%',startangle = 90,explode = (0.1,0),colors = colors)\n",
    "plt.title('Sex');\n",
    "\n",
    "plt.subplot(3,2,2)\n",
    "plt.pie(cp,labels = ['ASY', 'NAP', 'ATA', 'TA'],autopct='%1.1f%%',startangle = 90,explode = (0,0.1,0.1,0.1))\n",
    "plt.title('ChestPainType');\n",
    "\n",
    "plt.subplot(3,2,3)\n",
    "plt.pie(fbs,labels = ['FBS < 120 mg/dl','FBS > 120 mg/dl'],autopct='%1.1f%%',startangle = 90,explode = (0.1,0),colors = colors)\n",
    "plt.title('FastingBS');\n",
    "\n",
    "plt.subplot(3,2,4)\n",
    "plt.pie(restecg,labels = ['Normal','ST','LVH'],autopct='%1.1f%%',startangle = 90,explode = (0,0.1,0.1))\n",
    "plt.title('RestingECG');\n",
    "\n",
    "plt.subplot(3,2,5)\n",
    "plt.pie(exang,labels = ['Angina','No Angina'],autopct='%1.1f%%',startangle = 90,explode = (0.1,0),colors = colors)\n",
    "plt.title('ExerciseAngina');\n",
    "\n",
    "plt.subplot(3,2,6)\n",
    "plt.pie(slope,labels = ['Flat','Up','Down'],autopct='%1.1f%%',startangle = 90,explode = (0,0.1,0.1))\n",
    "plt.title('ST_Slope');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook HeartFailurePredictionDataset.ipynb to pdf\n",
      "[NbConvertApp] Support files will be in HeartFailurePredictionDataset_files/\n",
      "[NbConvertApp] Making directory ./HeartFailurePredictionDataset_files\n",
      "[NbConvertApp] Making directory ./HeartFailurePredictionDataset_files\n",
      "[NbConvertApp] Making directory ./HeartFailurePredictionDataset_files\n",
      "[NbConvertApp] Making directory ./HeartFailurePredictionDataset_files\n",
      "[NbConvertApp] Making directory ./HeartFailurePredictionDataset_files\n",
      "[NbConvertApp] Making directory ./HeartFailurePredictionDataset_files\n",
      "[NbConvertApp] Making directory ./HeartFailurePredictionDataset_files\n",
      "[NbConvertApp] Making directory ./HeartFailurePredictionDataset_files\n",
      "[NbConvertApp] Making directory ./HeartFailurePredictionDataset_files\n",
      "[NbConvertApp] Making directory ./HeartFailurePredictionDataset_files\n",
      "[NbConvertApp] Making directory ./HeartFailurePredictionDataset_files\n",
      "[NbConvertApp] Making directory ./HeartFailurePredictionDataset_files\n",
      "[NbConvertApp] Making directory ./HeartFailurePredictionDataset_files\n",
      "[NbConvertApp] Making directory ./HeartFailurePredictionDataset_files\n",
      "[NbConvertApp] Making directory ./HeartFailurePredictionDataset_files\n",
      "[NbConvertApp] Making directory ./HeartFailurePredictionDataset_files\n",
      "[NbConvertApp] Making directory ./HeartFailurePredictionDataset_files\n",
      "[NbConvertApp] Making directory ./HeartFailurePredictionDataset_files\n",
      "[NbConvertApp] Making directory ./HeartFailurePredictionDataset_files\n",
      "[NbConvertApp] Making directory ./HeartFailurePredictionDataset_files\n",
      "[NbConvertApp] Writing 172498 bytes to notebook.tex\n",
      "[NbConvertApp] Building PDF\n",
      "[NbConvertApp] Running xelatex 3 times: ['xelatex', 'notebook.tex', '-quiet']\n",
      "[NbConvertApp] Running bibtex 1 time: ['bibtex', 'notebook']\n",
      "[NbConvertApp] WARNING | bibtex had problems, most likely because there were no citations\n",
      "[NbConvertApp] PDF successfully created\n",
      "[NbConvertApp] Writing 794574 bytes to HeartFailurePredictionDataset.pdf\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbconvert --to pdf HeartFailurePredictionDataset.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
